{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion detection on NSL-KDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my try with [NSL-KDD](http://www.unb.ca/research/iscx/dataset/iscx-NSL-KDD-dataset.html) dataset, which is an improved version of well-known [KDD'99](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset. I've used Python, Scikit-learn and PySpark via [ready-to-run Jupyter applications in Docker](https://github.com/jupyter/docker-stacks).\n",
    "\n",
    "I've tried a variety of approaches to deal with this dataset. Here are presented some of them.\n",
    "\n",
    "Note: I've had to build my own all-spark docker image for trying Apache Spark 2.0 at that moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Task description summary](#1.-Task-description-summary)\n",
    "2. [Data loading](#2.-Data-loading)\n",
    "3. [Exploratory Data Analysis](#3.-Exploratory-Data-Analysis)\n",
    "4. [One Hot Encoding for categorical variables](#4.-One-Hot-Encoding-for-categorical-variables)\n",
    "5. [Feature Selection using Attribute Ratio](#5.-Feature-Selection-using-Attribute-Ratio)\n",
    "6. [Data preparation](#6.-Data-preparation)\n",
    "7. [Visualization via PCA](#7.-Visualization-via-PCA)\n",
    "8. [KMeans clustering with Random Forest Classifiers](#8.-KMeans-clustering-with-Random-Forest-Classifiers)\n",
    "9. [Gaussian Mixture clustering with Random Forest Classifiers](#9.-Gaussian-Mixture-clustering-with-Random-Forest-Classifiers)\n",
    "10. [Supervised approach for dettecting each type of attacks separately](#10.-Supervised-approach-for-dettecting-each-type-of-attacks-separately)\n",
    "11. [Ensembling experiments](#11.-Ensembling-experiments)\n",
    "12. [Results summary](#12.-Results-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task description summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software to detect network intrusions protects a computer network from unauthorized users, including perhaps insiders. The intrusion detector learning task is to build a predictive model (i.e. a classifier) capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections.\n",
    "\n",
    "A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol. Each connection is labeled as either normal, or as an attack, with exactly one specific attack type. Each connection record consists of about 100 bytes.\n",
    "\n",
    "Attacks fall into four main categories:\n",
    "\n",
    "- DOS: denial-of-service, e.g. syn flood;\n",
    "- R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "- U2R: unauthorized access to local superuser (root) privileges, e.g., various ''buffer overflow'' attacks;\n",
    "- probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data. This makes the task more realistic. Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants.  The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only.\n",
    "\n",
    "The complete task description could be found [here](http://kdd.ics.uci.edu/databases/kddcup99/task.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSL-KDD dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NSL-KDD](http://www.unb.ca/research/iscx/dataset/iscx-NSL-KDD-dataset.html) is a data set suggested to solve some of the inherent problems of the [KDD'99](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) data set.\n",
    "\n",
    "The NSL-KDD data set has the following advantages over the original KDD data set:\n",
    "- It does not include redundant records in the train set, so the classifiers will not be biased towards more frequent records.\n",
    "- There is no duplicate records in the proposed test sets; therefore, the performance of the learners are not biased by the methods which have better detection rates on the frequent records.\n",
    "- The number of selected records from each difficultylevel group is inversely proportional to the percentage of records in the original KDD data set. As a result, the classification rates of distinct machine learning methods vary in a wider range, which makes it more efficient to have an accurate evaluation of different learning techniques.\n",
    "- The number of records in the train and test sets are reasonable, which makes it affordable to run the experiments on the complete set without the need to randomly select a small portion. Consequently, evaluation results of different research works will be consistent and comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here are some imports that are used along this notebook\n",
    "import math\n",
    "import itertools\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "gt0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c66f2dab68ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Creating local SparkContext with 8 threads and SQLContext based on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'local[8]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "# Creating local SparkContext with 8 threads and SQLContext based on it\n",
    "sc = pyspark.SparkContext(master='local[8]')\n",
    "sc.setLogLevel('INFO')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, split, col\n",
    "import pyspark.sql.functions as sql\n",
    "\n",
    "train20_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTrain+_20Percent.txt\"\n",
    "train_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTrain+.txt\"\n",
    "test_nsl_kdd_dataset_path = \"NSL_KDD_Dataset/KDDTest+.txt\"\n",
    "\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
    "\n",
    "nominal_inx = [1, 2, 3]\n",
    "binary_inx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
    "\n",
    "nominal_cols = col_names[nominal_inx].tolist()\n",
    "binary_cols = col_names[binary_inx].tolist()\n",
    "numeric_cols = col_names[numeric_inx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to load dataset and divide it into 8 partitions\n",
    "def load_dataset(path):\n",
    "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
    "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
    "                    col('duration').cast(DoubleType()),\n",
    "                    col('protocol_type').cast(StringType()),\n",
    "                    col('service').cast(StringType()),\n",
    "                    col('flag').cast(StringType()),\n",
    "                    col('src_bytes').cast(DoubleType()),\n",
    "                    col('dst_bytes').cast(DoubleType()),\n",
    "                    col('land').cast(DoubleType()),\n",
    "                    col('wrong_fragment').cast(DoubleType()),\n",
    "                    col('urgent').cast(DoubleType()),\n",
    "                    col('hot').cast(DoubleType()),\n",
    "                    col('num_failed_logins').cast(DoubleType()),\n",
    "                    col('logged_in').cast(DoubleType()),\n",
    "                    col('num_compromised').cast(DoubleType()),\n",
    "                    col('root_shell').cast(DoubleType()),\n",
    "                    col('su_attempted').cast(DoubleType()),\n",
    "                    col('num_root').cast(DoubleType()),\n",
    "                    col('num_file_creations').cast(DoubleType()),\n",
    "                    col('num_shells').cast(DoubleType()),\n",
    "                    col('num_access_files').cast(DoubleType()),\n",
    "                    col('num_outbound_cmds').cast(DoubleType()),\n",
    "                    col('is_host_login').cast(DoubleType()),\n",
    "                    col('is_guest_login').cast(DoubleType()),\n",
    "                    col('count').cast(DoubleType()),\n",
    "                    col('srv_count').cast(DoubleType()),\n",
    "                    col('serror_rate').cast(DoubleType()),\n",
    "                    col('srv_serror_rate').cast(DoubleType()),\n",
    "                    col('rerror_rate').cast(DoubleType()),\n",
    "                    col('srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('same_srv_rate').cast(DoubleType()),\n",
    "                    col('diff_srv_rate').cast(DoubleType()),\n",
    "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_count').cast(DoubleType()),\n",
    "                    col('dst_host_srv_count').cast(DoubleType()),\n",
    "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('labels').cast(StringType())))\n",
    "\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of data preparation is deviding connections into normal and attack classes based on 'labels' column. Then attacks are splitted to four main categories: DoS, Probe, R2L and U2R. After this, all of those categories are indexed. Also, ID column is added to simplify work with clustered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "\n",
    "# Dictionary that contains mapping of various attacks to the four main categories\n",
    "attack_dict = {\n",
    "    'normal': 'normal',\n",
    "    \n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'worm': 'R2L',\n",
    "    \n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'ps': 'U2R',    \n",
    "    'sqlattack': 'U2R',\n",
    "    'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
    "\n",
    "class Labels2Converter(Transformer):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(Labels2Converter, self).__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
    "     \n",
    "class Labels5Converter(Transformer):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(Labels5Converter, self).__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
    "    \n",
    "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
    "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
    "\n",
    "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
    "\n",
    "labels2 = ['normal', 'attack']\n",
    "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "labels_col = 'labels2_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "t0 = time()\n",
    "train_df = load_dataset(train_nsl_kdd_dataset_path)\n",
    "\n",
    "# Fitting preparation pipeline\n",
    "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading test data\n",
    "t0 = time()\n",
    "test_df = load_dataset(test_nsl_kdd_dataset_path)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some descriptive statistics of available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Labels columns\n",
    "(train_df.groupby('labels2').count().show())\n",
    "(train_df.groupby('labels5').count().sort(sql.desc('count')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(test_df.groupby('labels2').count().show())\n",
    "(test_df.groupby('labels5').count().sort(sql.desc('count')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 'protocol_type' nominal column\n",
    "(train_df.crosstab(nominal_cols[0], 'labels2').sort(sql.asc(nominal_cols[0] + '_labels2')).show())\n",
    "(train_df.crosstab(nominal_cols[0], 'labels5').sort(sql.asc(nominal_cols[0] + '_labels5')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'service' nominal column\n",
    "print(train_df.select(nominal_cols[1]).distinct().count())\n",
    "(train_df.crosstab(nominal_cols[1], 'labels2').sort(sql.asc(nominal_cols[1] + '_labels2')).show(n=70))\n",
    "(train_df.crosstab(nominal_cols[1], 'labels5').sort(sql.asc(nominal_cols[1] + '_labels5')).show(n=70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 'flag' nominal column\n",
    "print(train_df.select(nominal_cols[2]).distinct().count())\n",
    "(train_df.crosstab(nominal_cols[2], 'labels2').sort(sql.asc(nominal_cols[2] + '_labels2')).show())\n",
    "(train_df.crosstab(nominal_cols[2], 'labels5').sort(sql.asc(nominal_cols[2] + '_labels5')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Binary columns\n",
    "(train_df.select(binary_cols).describe().toPandas().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'su_attempted' should be a binary feature, but has 3 values\n",
    "(train_df.crosstab('su_attempted', 'labels2').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# '2.0' value is replaced to '0.0' for both train and test datasets\n",
    "train_df = train_df.replace(2.0, 0.0, 'su_attempted')\n",
    "test_df = test_df.replace(2.0, 0.0, 'su_attempted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "print(len(numeric_cols))\n",
    "(train_df.select(numeric_cols).describe().toPandas().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_df.crosstab('num_outbound_cmds', 'labels2').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 'num_outbound_cmds' feature takes only 0.0 values, so it is dropped as redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop('num_outbound_cmds')\n",
    "test_df = test_df.drop('num_outbound_cmds')\n",
    "numeric_cols.remove('num_outbound_cmds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented code below is related to removing highly correlated features. However, it hasen't been tested a lot yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.stat import Statistics\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# t0 = time()\n",
    "# stat_assembler = VectorAssembler(\n",
    "#                 inputCols=numeric_cols,\n",
    "#                 outputCol='features')\n",
    "\n",
    "# stat_rdd = stat_assembler.transform(train_df).rdd.map(lambda row: row['features'].toArray())\n",
    "\n",
    "# pearson_corr = Statistics.corr(stat_rdd, method='pearson')\n",
    "# spearman_corr = Statistics.corr(stat_rdd, method='spearman')\n",
    "\n",
    "# print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# f, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# ax1.set_title(\"Pearson\")\n",
    "# ax2.set_title(\"Spearman\")\n",
    "# sns.heatmap(pearson_corr, ax=ax1)\n",
    "# sns.heatmap(spearman_corr, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inx_correlated_to_delete = [8, 15, 28, 17, 29]\n",
    "\n",
    "# for inx in inx_correlated_to_delete:\n",
    "#     train_df = train_df.drop(numeric_cols[inx])\n",
    "#     test_df = test_df.drop(numeric_cols[inx])\n",
    "\n",
    "# numeric_cols = [col for inx, col in enumerate(numeric_cols) if inx not in inx_correlated_to_delete]\n",
    "\n",
    "# train_df = train_df.cache()\n",
    "# test_df = test_df.cache()\n",
    "# print(train_df.count())\n",
    "# print(test_df.count())\n",
    "# print(len(numeric_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One Hot Encoding for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding (OHE) is used for treating categorical variables. Custom function is created for demonstration purposes. However, it could be easily replaced by PySpark OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ohe_vec(cat_dict, row):\n",
    "    vec = np.zeros(len(cat_dict))\n",
    "    vec[cat_dict[row]] = float(1.0)\n",
    "    return vec.tolist()\n",
    "\n",
    "def ohe(df, nominal_col):\n",
    "    categories = (df.select(nominal_col)\n",
    "                    .distinct()\n",
    "                    .rdd.map(lambda row: row[0])\n",
    "                    .collect())\n",
    "    \n",
    "    cat_dict = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    udf_ohe_vec = udf(lambda row: ohe_vec(cat_dict, row), \n",
    "                      StructType([StructField(cat, DoubleType(), False) for cat in categories]))\n",
    "    \n",
    "    df = df.withColumn(nominal_col + '_ohe', udf_ohe_vec(col(nominal_col))).cache()\n",
    "    \n",
    "    nested_cols = [nominal_col + '_ohe.' + cat for cat in categories]\n",
    "    ohe_cols = [nominal_col + '_' + cat for cat in categories]\n",
    "        \n",
    "    for new, old in zip(ohe_cols, nested_cols):\n",
    "        df = df.withColumn(new, col(old))\n",
    "\n",
    "    df = df.drop(nominal_col + '_ohe')\n",
    "                   \n",
    "    return df, ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "train_ohe_cols = []\n",
    "\n",
    "train_df, train_ohe_col0 = ohe(train_df, nominal_cols[0])\n",
    "train_ohe_cols += train_ohe_col0\n",
    "\n",
    "train_df, train_ohe_col1 = ohe(train_df, nominal_cols[1])\n",
    "train_ohe_cols += train_ohe_col1\n",
    "\n",
    "train_df, train_ohe_col2 = ohe(train_df, nominal_cols[2])\n",
    "train_ohe_cols += train_ohe_col2\n",
    "\n",
    "binary_cols += train_ohe_cols\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom list of test binary cols is used as test dataset could contain additional categories for 'service' and 'flag' features. However, those additional categories aren't used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "test_ohe_cols = []\n",
    "\n",
    "test_df, test_ohe_col0_names = ohe(test_df, nominal_cols[0])\n",
    "test_ohe_cols += test_ohe_col0_names\n",
    "\n",
    "test_df, test_ohe_col1_names = ohe(test_df, nominal_cols[1])\n",
    "test_ohe_cols += test_ohe_col1_names\n",
    "\n",
    "test_df, test_ohe_col2_names = ohe(test_df, nominal_cols[2])\n",
    "test_ohe_cols += test_ohe_col2_names\n",
    "\n",
    "test_binary_cols = col_names[binary_inx].tolist() + test_ohe_cols\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection using Attribute Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Ratio approach is used for feature selection purposes. This approach was described by Hee-su Chae and Sang Hyun Choi in [Feature Selection for efficient Intrusion Detection using Attribute Ratio](http://www.naun.org/main/UPress/cc/2014/a102019-106.pdf) and [Feature Selection for Intrusion Detection using NSL-KDD](http://www.wseas.us/e-library/conferences/2013/Nanjing/ACCIS/ACCIS-30.pdf)\n",
    "\n",
    "This approach is also used for nominal variables as they were encoded as binary variables above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a possible to have 'null' values because binary features could have Frequency(0) = 0, those 'null' values are replaced with 1000.0 (magic number). For NSL KDD dataset it is related only for 'protocol_type_tcp' ohe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getAttributeRatio(df, numericCols, binaryCols, labelCol):\n",
    "    ratio_dict = {}\n",
    "    \n",
    "    if numericCols:\n",
    "        avg_dict = (df\n",
    "                .select(list(map(lambda c: sql.avg(c).alias(c), numericCols)))\n",
    "                .first()\n",
    "                .asDict())\n",
    "\n",
    "        ratio_dict.update(df\n",
    "                .groupBy(labelCol)\n",
    "                .avg(*numericCols)\n",
    "                .select(list(map(lambda c: sql.max(col('avg(' + c + ')')/avg_dict[c]).alias(c), numericCols)))\n",
    "                .fillna(0.0)\n",
    "                .first()\n",
    "                .asDict())\n",
    "    \n",
    "    if binaryCols:\n",
    "        ratio_dict.update((df\n",
    "                .groupBy(labelCol)\n",
    "                .agg(*list(map(lambda c: (sql.sum(col(c))/(sql.count(col(c)) - sql.sum(col(c)))).alias(c), binaryCols)))\n",
    "                .fillna(1000.0)\n",
    "                .select(*list(map(lambda c: sql.max(col(c)).alias(c), binaryCols)))\n",
    "                .first()\n",
    "                .asDict()))\n",
    "        \n",
    "    return OrderedDict(sorted(ratio_dict.items(), key=lambda v: -v[1]))\n",
    "\n",
    "def selectFeaturesByAR(ar_dict, min_ar):\n",
    "    return [f for f in ar_dict.keys() if ar_dict[f] >= min_ar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "ar_dict = getAttributeRatio(train_df, numeric_cols, binary_cols, 'labels5')\n",
    "\n",
    "print(len(ar_dict))\n",
    "print(time() - t0)\n",
    "ar_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standartization is necessary as a lot of distance based algorithms are used below. Custom standartization is created for demonstration purposes, so it could be easily replaced by PySpark StandardScaler. Note that data is sparse, so it is reasonable to not substract mean for avoiding violating sparsity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "avg_dict = (train_df.select(list(map(lambda c: sql.avg(c).alias(c), numeric_cols))).first().asDict())\n",
    "std_dict = (train_df.select(list(map(lambda c: sql.stddev(c).alias(c), numeric_cols))).first().asDict())\n",
    "\n",
    "def standardizer(column):\n",
    "    return ((col(column) - avg_dict[column])/std_dict[column]).alias(column)\n",
    "\n",
    "# Standardizer without mean\n",
    "# def standardizer(column):\n",
    "#     return (col(column)/std_dict[column]).alias(column)\n",
    "\n",
    "train_scaler = [*binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
    "test_scaler = [*test_binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
    "\n",
    "scaled_train_df = (train_df.select(train_scaler).cache())\n",
    "scaled_test_df = (test_df.select(test_scaler).cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler is used for combining a given list of columns into a single vector column. Then VectorIndexer is used for indexing categorical (binary) features. Indexing categorical features allows algorithms to treat them appropriately, improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=selectFeaturesByAR(ar_dict, 0.01), outputCol='raw_features')\n",
    "indexer = VectorIndexer(inputCol='raw_features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[assembler, indexer])\n",
    "prep_model = prep_pipeline.fit(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "scaled_train_df = (prep_model\n",
    "        .transform(scaled_train_df)\n",
    "        .select('id', 'indexed_features', 'labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "scaled_test_df = (prep_model \n",
    "        .transform(scaled_test_df)\n",
    "        .select('id', 'indexed_features','labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting seed for reproducibility\n",
    "seed = 4667979835606274383\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset is splitted into 80% train and 20% cross-validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = (scaled_train_df.randomSplit([0.8, 0.2], seed=seed))\n",
    "\n",
    "scaled_train_df = split[0].cache()\n",
    "scaled_cv_df = split[1].cache()\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_cv_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional \"result\" dataframes are used to collect probabilities and predictions from different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_cv_df = scaled_cv_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "res_test_df = scaled_test_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "prob_cols = []\n",
    "pred_cols = []\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different metrics from sklearn are used for evaluating results. The most important from them for this task are False positive Rate, Detection Rate and F1 score. \n",
    "As evaluating via sklearn requires to collect predicted and label columns to the driver, it will be replaced with PySpark metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels])\n",
    "    # Print header\n",
    "    print(\" \" * columnwidth, end=\"\\t\")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
    "        for j in range(len(labels)):\n",
    "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "def getPrediction(e):\n",
    "    return udf(lambda row: 1.0 if row >= e else 0.0, DoubleType())\n",
    "        \n",
    "def printReport(resDF, probCol, labelCol='labels2_index', e=None, labels=['normal', 'attack']):\n",
    "    if (e):\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (1.0 if row[probCol] >= e else 0.0, row[labelCol]))\n",
    "                                       .collect()))\n",
    "    else:\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (row[probCol], row[labelCol]))\n",
    "                                       .collect()))\n",
    "    \n",
    "    cm = metrics.confusion_matrix(predictionAndLabels[1], predictionAndLabels[0])\n",
    "    printCM(cm, labels)\n",
    "    print(\" \")\n",
    "    print(\"Accuracy = %g\" % (metrics.accuracy_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\"AUC = %g\" % (metrics.roc_auc_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\" \")\n",
    "    print(\"False Alarm Rate = %g\" % (cm[0][1]/(cm[0][0] + cm[0][1])))\n",
    "    print(\"Detection Rate = %g\" % (cm[1][1]/(cm[1][1] + cm[1][0])))\n",
    "    print(\"F1 score = %g\" % (metrics.f1_score(predictionAndLabels[1], predictionAndLabels[0], labels)))\n",
    "    print(\" \")\n",
    "    print(metrics.classification_report(predictionAndLabels[1], predictionAndLabels[0]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization via PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA algorithm is used for visualization purposes. It's also used later as preprocessing for Gaussian Mixture clustering.\n",
    "\n",
    "First graph shows 'attack' vs 'normal' labels, second graph shows 4 different types of attacks vs normal connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "t0 = time()\n",
    "pca_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", names=selectFeaturesByAR(ar_dict, 0.05))\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_pipeline = Pipeline(stages=[pca_slicer, pca])\n",
    "\n",
    "pca_train_df = pca_pipeline.fit(scaled_train_df).transform(scaled_train_df)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "viz_train_data = np.array(pca_train_df.rdd.map(lambda row: [*row['pca_features'], row['labels2_index'], row['labels5_index']]).collect())\n",
    "plt.figure()\n",
    "plt.scatter(x=viz_train_data[:,0], y=viz_train_data[:,1], c=viz_train_data[:,2], cmap=\"Set1\")\n",
    "plt.figure()\n",
    "plt.scatter(x=viz_train_data[:,0], y=viz_train_data[:,1], c=viz_train_data[:,3], cmap=\"Set1\")\n",
    "plt.show()\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KMeans clustering with Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the first approach is to clusterize data into clusters and then train different Random Forest classifiers for each of the clusters. As Random Forest returns probabilities, it is possible to improve detection rate for a new types of attacks by adjusting threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As KMeans cannot truly handle binary/categorical features only numeric features are used for clustarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans_prob_col = 'kmeans_rf_prob'\n",
    "kmeans_pred_col = 'kmeans_rf_pred'\n",
    "\n",
    "prob_cols.append(kmeans_prob_col)\n",
    "pred_cols.append(kmeans_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# KMeans clustrering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "t0 = time()\n",
    "kmeans_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", \n",
    "                             names=list(set(selectFeaturesByAR(ar_dict, 0.1)).intersection(numeric_cols)))\n",
    "\n",
    "kmeans = KMeans(k=8, initSteps=25, maxIter=100, featuresCol=\"features\", predictionCol=\"cluster\", seed=seed)\n",
    "\n",
    "kmeans_pipeline = Pipeline(stages=[kmeans_slicer, kmeans])\n",
    "\n",
    "kmeans_model = kmeans_pipeline.fit(scaled_train_df)\n",
    "\n",
    "kmeans_train_df = kmeans_model.transform(scaled_train_df).cache()\n",
    "kmeans_cv_df = kmeans_model.transform(scaled_cv_df).cache()\n",
    "kmeans_test_df = kmeans_model.transform(scaled_test_df).cache()\n",
    "\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function for describing the contents of the clusters \n",
    "def getClusterCrosstab(df, clusterCol='cluster'):\n",
    "    return (df.crosstab(clusterCol, 'labels2')\n",
    "              .withColumn('count', col('attack') + col('normal'))\n",
    "              .withColumn(clusterCol + '_labels2', col(clusterCol + '_labels2').cast('int'))\n",
    "              .sort(col(clusterCol +'_labels2').asc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeans_crosstab = getClusterCrosstab(kmeans_train_df).cache()\n",
    "kmeans_crosstab.show(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustres are splitted into two categories. Frist category contains clusters that have both 'attack' and 'normal' connections and have more than 25 connections. For the first category Random Forest classifiers are aplied. Second category contains all other clusters and maps cluster to 'attack' or 'normal' based on majority. All clusters that contains less or equal than 25 connections are treated as outliers and are mapped to 'attack' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function for splitting clusters\n",
    "def splitClusters(crosstab):\n",
    "    exp = ((col('count') > 25) & (col('attack') > 0) & (col('normal') > 0))\n",
    "\n",
    "    cluster_rf = (crosstab\n",
    "        .filter(exp).rdd\n",
    "        .map(lambda row: (int(row['cluster_labels2']), [row['count'], row['attack']/row['count']]))\n",
    "        .collectAsMap())\n",
    "\n",
    "    cluster_mapping = (crosstab\n",
    "        .filter(~exp).rdd\n",
    "        .map(lambda row: (int(row['cluster_labels2']), 1.0 if (row['count'] <= 25) | (row['normal'] == 0) else 0.0))\n",
    "        .collectAsMap())\n",
    "    \n",
    "    return cluster_rf, cluster_mapping\n",
    "\n",
    "kmeans_cluster_rf, kmeans_cluster_mapping = splitClusters(kmeans_crosstab)\n",
    "\n",
    "print(len(kmeans_cluster_rf), len(kmeans_cluster_mapping))\n",
    "print(kmeans_cluster_mapping)\n",
    "kmeans_cluster_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# This function returns Random Forest models for provided clusters\n",
    "def getClusterModels(df, cluster_rf):\n",
    "    cluster_models = {}\n",
    "\n",
    "    labels_col = 'labels2_cl_index'\n",
    "    labels2_indexer.setOutputCol(labels_col)\n",
    "\n",
    "    rf_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"rf_features\", \n",
    "                             names=selectFeaturesByAR(ar_dict, 0.05))\n",
    "\n",
    "    for cluster in cluster_rf.keys():\n",
    "        t1 = time()\n",
    "        rf_classifier = RandomForestClassifier(labelCol=labels_col, featuresCol='rf_features', seed=seed,\n",
    "                                               numTrees=500, maxDepth=20, featureSubsetStrategy=\"sqrt\")\n",
    "        \n",
    "        rf_pipeline = Pipeline(stages=[labels2_indexer, rf_slicer, rf_classifier])\n",
    "        cluster_models[cluster] = rf_pipeline.fit(df.filter(col('cluster') == cluster))\n",
    "        print(\"Finished %g cluster in %g ms\" % (cluster, time() - t1))\n",
    "        \n",
    "    return cluster_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This utility function helps to get predictions/probabilities for the new data and return them into one dataframe\n",
    "def getProbabilities(df, probCol, cluster_mapping, cluster_models):\n",
    "    pred_df = (sqlContext.createDataFrame([], StructType([\n",
    "                    StructField('id', LongType(), False),\n",
    "                    StructField(probCol, DoubleType(), False)])))\n",
    "    \n",
    "    udf_map = udf(lambda cluster: cluster_mapping[cluster], DoubleType())\n",
    "    pred_df = pred_df.union(df.filter(col('cluster').isin(list(cluster_mapping.keys())))\n",
    "                            .withColumn(probCol, udf_map(col('cluster')))\n",
    "                            .select('id', probCol))\n",
    "\n",
    "                                       \n",
    "    for k in cluster_models.keys():\n",
    "        maj_label = cluster_models[k].stages[0].labels[0]\n",
    "        udf_remap_prob = udf(lambda row: float(row[0]) if (maj_label == 'attack') else float(row[1]), DoubleType())\n",
    "\n",
    "        pred_df = pred_df.union(cluster_models[k]\n",
    "                         .transform(df.filter(col('cluster') == k))\n",
    "                         .withColumn(probCol, udf_remap_prob(col('probability')))\n",
    "                         .select('id', probCol))\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Random Forest classifiers for each of the clusters\n",
    "t0 = time()\n",
    "kmeans_cluster_models = getClusterModels(kmeans_train_df, kmeans_cluster_rf)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(kmeans_prob_col)\n",
    "             .join(getProbabilities(kmeans_cv_df, kmeans_prob_col, kmeans_cluster_mapping, kmeans_cluster_models), 'id')\n",
    "             .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(kmeans_prob_col)\n",
    "               .join(getProbabilities(kmeans_test_df, kmeans_prob_col, kmeans_cluster_mapping, kmeans_cluster_models), 'id')\n",
    "               .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As CV data is from the same distribution as the train data it isn't needed to adjust threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df, kmeans_prob_col, e=0.5, labels=labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because test data is from the different distribution and it is expected to face unseen attack types, it makes sence to adjust a probability threshold to something like 0.01 for attack connections (0.99 for normal connections). For this approach it gives around ~98-99% Detection Rate with around ~14-15% of False Alarm Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df, kmeans_prob_col, e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(kmeans_pred_col, getPrediction(0.5)(col(kmeans_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(kmeans_pred_col, getPrediction(0.01)(col(kmeans_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gaussian Mixture clustering with Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this approach is to clusterize data into clusters via Gaussian Mixture and then train different Random Forest classifiers for each of the clusters. Gaussian Mixture produces a diffirent clustering than KMeans, so results from both approaches could be combine for improving performance. As Gaussian Mixture clustering doesn't work well on high-demensional data PCA algorithm is used for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gm_prob_col = 'gm_rf_prob'\n",
    "gm_pred_col = 'gm_rf_pred'\n",
    "\n",
    "prob_cols.append(gm_prob_col)\n",
    "pred_cols.append(gm_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gaussian Mixture clustering\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "t0 = time()\n",
    "gm = GaussianMixture(k=8, maxIter=150, seed=seed, featuresCol=\"pca_features\", \n",
    "                     predictionCol=\"cluster\", probabilityCol=\"gm_prob\")\n",
    "\n",
    "gm_pipeline = Pipeline(stages=[pca_slicer, pca, gm])\n",
    "gm_model = gm_pipeline.fit(scaled_train_df)\n",
    "\n",
    "gm_train_df = gm_model.transform(scaled_train_df).cache()\n",
    "gm_cv_df = gm_model.transform(scaled_cv_df).cache()\n",
    "gm_test_df = gm_model.transform(scaled_test_df).cache()\n",
    "\n",
    "gm_params = (gm_model.stages[2].gaussiansDF.rdd\n",
    "                  .map(lambda row: [row['mean'].toArray(), row['cov'].toArray()])\n",
    "                  .collect())\n",
    "gm_weights = gm_model.stages[2].weights\n",
    "\n",
    "print(gm_train_df.count())\n",
    "print(gm_cv_df.count())\n",
    "print(gm_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Description of the contents of the clusters \n",
    "gm_crosstab = getClusterCrosstab(gm_train_df).cache()\n",
    "gm_crosstab.show(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Splitting clusters\n",
    "gm_cluster_rf, gm_cluster_mapping = splitClusters(gm_crosstab)\n",
    "\n",
    "print(len(gm_cluster_rf), len(gm_cluster_mapping))\n",
    "print(gm_cluster_mapping)\n",
    "gm_cluster_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Random Forest classifiers for each of the clusters\n",
    "t0 = time()\n",
    "gm_cluster_models = getClusterModels(gm_train_df, gm_cluster_rf)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(gm_prob_col)\n",
    "             .join(getProbabilities(gm_cv_df, gm_prob_col, gm_cluster_mapping, gm_cluster_models), 'id')\n",
    "             .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(gm_prob_col)\n",
    "               .join(getProbabilities(gm_test_df, gm_prob_col, gm_cluster_mapping, gm_cluster_models), 'id')\n",
    "               .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df, gm_prob_col, e=0.5, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df, gm_prob_col, e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(gm_pred_col, getPrediction(0.5)(col(gm_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(gm_pred_col, getPrediction(0.01)(col(gm_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Supervised approach for dettecting each type of attacks separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the following approach is training Random Forest Classifiers for each of four major 'attack' categories separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 DoS and normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dos_prob_col = 'dos_prob'\n",
    "dos_pred_col = 'dos_pred'\n",
    "\n",
    "prob_cols.append(dos_prob_col)\n",
    "pred_cols.append(dos_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dos_exp = (col('labels5') == 'DoS') | (col('labels5') == 'normal')\n",
    "dos_train_df = (scaled_train_df.filter(dos_exp).cache())\n",
    "\n",
    "print(dos_train_df.count())\n",
    "(dos_train_df\n",
    "     .groupby('labels5')\n",
    "     .count()\n",
    "     .sort(sql.desc('count'))\n",
    "     .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented code below is related to undersampling 'normal' connections. It could give better results. However, it hasen't been tested a lot yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dos_train_df = dos_train_df.sampleBy('labels5', fractions={'normal': 45927./67343, 'DoS': 1.0}).cache()\n",
    "\n",
    "# print(dos_train_df.count())\n",
    "# (dos_train_df\n",
    "#      .groupby('labels5')\n",
    "#      .count()\n",
    "#      .sort(sql.desc('count'))\n",
    "#      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffirent AR feature selection is used as only normal and DoS connections are treated. Note that train dataframe without standartization is used for getting Attribute Ratio dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "dos_ar_dict = getAttributeRatio(train_df.filter(dos_exp), numeric_cols, binary_cols, 'labels5')\n",
    "\n",
    "print(time() - t0)\n",
    "dos_ar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "dos_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", \n",
    "                          names=selectFeaturesByAR(dos_ar_dict, 0.05))\n",
    "\n",
    "dos_rf = RandomForestClassifier(labelCol=labels_col, featuresCol='features', featureSubsetStrategy='sqrt',\n",
    "                                numTrees=500, maxDepth=20, seed=seed)\n",
    "\n",
    "dos_rf_pipeline = Pipeline(stages=[dos_slicer, dos_rf])\n",
    "dos_rf_model = dos_rf_pipeline.fit(dos_train_df)\n",
    "\n",
    "dos_cv_df = dos_rf_model.transform(scaled_cv_df).cache()\n",
    "dos_test_df = dos_rf_model.transform(scaled_test_df).cache()\n",
    "print(dos_cv_df.count())\n",
    "print(dos_test_df.count())\n",
    "\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(dos_prob_col)\n",
    "             .join(dos_cv_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', dos_prob_col]),\n",
    "                    'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(dos_prob_col)\n",
    "               .join(dos_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', dos_prob_col]),\n",
    "                    'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first report shows performance of classification for 'normal' and 'DoS' labels, the second report shows performance for the whole data with adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df.filter(dos_exp), probCol=dos_prob_col, e=0.5, labels=['normal', 'DoS'])\n",
    "printReport(res_cv_df, probCol=dos_prob_col, e=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df.filter(dos_exp), probCol=dos_prob_col, e=0.5, labels=['normal', 'DoS'])\n",
    "printReport(res_test_df, probCol=dos_prob_col, e=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(dos_pred_col, getPrediction(0.05)(col(dos_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(dos_pred_col, getPrediction(0.01)(col(dos_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Probe and normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probe_prob_col = 'probe_prob'\n",
    "probe_pred_col = 'probe_pred'\n",
    "\n",
    "prob_cols.append(probe_prob_col)\n",
    "pred_cols.append(probe_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probe_exp = (col('labels5') == 'Probe') | (col('labels5') == 'normal')\n",
    "probe_train_df = (scaled_train_df.filter(probe_exp).cache())\n",
    "\n",
    "print(probe_train_df.count())\n",
    "(probe_train_df\n",
    "     .groupby('labels5')\n",
    "     .count()\n",
    "     .sort(sql.desc('count'))\n",
    "     .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented code below is related to undersampling 'normal' connections. It could give better results. However, it hasen't been tested a lot yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# probe_train_df = probe_train_df.sampleBy('labels5', fractions={'normal': 9274./53789, 'Probe': 1.0}).cache()\n",
    "\n",
    "# print(probe_train_df.count())\n",
    "# (probe_train_df\n",
    "#      .groupby('labels5')\n",
    "#      .count()\n",
    "#      .sort(sql.desc('count'))\n",
    "#      .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffirent AR feature selection is used as only normal and Probe connections are treated. Note that train dataframe without standartization is used for getting Attribute Ratio dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "probe_ar_dict = getAttributeRatio(train_df.filter(probe_exp), numeric_cols, binary_cols, 'labels5')\n",
    "\n",
    "print(time() - t0)\n",
    "probe_ar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "probe_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\",\n",
    "                            names=selectFeaturesByAR(probe_ar_dict, 0.05))\n",
    "\n",
    "probe_rf = RandomForestClassifier(labelCol=labels_col, featuresCol='features', featureSubsetStrategy='sqrt',\n",
    "                                  numTrees=500, maxDepth=20, seed=seed)\n",
    "probe_rf_pipeline = Pipeline(stages=[probe_slicer, probe_rf])\n",
    "\n",
    "probe_rf_model = probe_rf_pipeline.fit(probe_train_df)\n",
    "\n",
    "probe_cv_df = probe_rf_model.transform(scaled_cv_df).cache()\n",
    "probe_test_df = probe_rf_model.transform(scaled_test_df).cache()\n",
    "\n",
    "print(probe_cv_df.count())\n",
    "print(probe_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(probe_prob_col)\n",
    "             .join(probe_cv_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', probe_prob_col]), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(probe_prob_col)\n",
    "               .join(probe_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', probe_prob_col]), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first report shows performance of classification for 'normal' and 'Probe' labels, the second report shows performance for the whole data with adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df.filter(probe_exp), probCol=probe_prob_col, e=0.5, labels=['normal', 'Probe'])\n",
    "printReport(res_cv_df, probCol=probe_prob_col, e=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df.filter(probe_exp), probCol=probe_prob_col, e=0.5, labels=['normal', 'Probe'])\n",
    "printReport(res_test_df, probCol=probe_prob_col, e=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(probe_pred_col, getPrediction(0.05)(col(probe_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(probe_pred_col, getPrediction(0.01)(col(probe_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 R2L, U2R and normal types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are a few examples of both R2L and U2R attack types and they have similar behaviour, they are combined into one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2l_u2r_prob_col = 'r2l_u2r_prob'\n",
    "r2l_u2r_pred_col = 'r2l_u2r_pred'\n",
    "\n",
    "prob_cols.append(r2l_u2r_prob_col)\n",
    "pred_cols.append(r2l_u2r_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2l_u2r_exp = (col('labels5') == 'R2L') | (col('labels5') == 'U2R') | (col('labels5') == 'normal')\n",
    "r2l_u2r_train_df = (scaled_train_df.filter(r2l_u2r_exp).cache())\n",
    "\n",
    "print(r2l_u2r_train_df.count())\n",
    "(r2l_u2r_train_df\n",
    "     .groupby('labels5')\n",
    "     .count()\n",
    "     .sort(sql.desc('count'))\n",
    "     .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffirent AR feature selection is used as only normal, R2L and U2R connections are treated. Note that train dataframe without standartization is used for getting Attribute Ratio dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "r2l_u2r_ar_dict = getAttributeRatio(train_df.filter(r2l_u2r_exp), numeric_cols, binary_cols, 'labels5')\n",
    "\n",
    "print(time() - t0)\n",
    "r2l_u2r_ar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "r2l_u2r_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\",\n",
    "                              names=selectFeaturesByAR(r2l_u2r_ar_dict, 0.05))\n",
    "\n",
    "r2l_u2r_rf = RandomForestClassifier(labelCol=labels_col, featuresCol='features', featureSubsetStrategy='sqrt',\n",
    "                                    numTrees=500, maxDepth=20, seed=seed)\n",
    "r2l_u2r_rf_pipeline = Pipeline(stages=[r2l_u2r_slicer, r2l_u2r_rf])\n",
    "\n",
    "r2l_u2r_rf_model = r2l_u2r_rf_pipeline.fit(r2l_u2r_train_df)\n",
    "\n",
    "r2l_u2r_cv_df = r2l_u2r_rf_model.transform(scaled_cv_df).cache()\n",
    "r2l_u2r_test_df = r2l_u2r_rf_model.transform(scaled_test_df).cache()\n",
    "print(r2l_u2r_cv_df.count())\n",
    "print(r2l_u2r_test_df.count())\n",
    "\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(r2l_u2r_prob_col)\n",
    "             .join(r2l_u2r_cv_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', r2l_u2r_prob_col]), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(r2l_u2r_prob_col)\n",
    "               .join(r2l_u2r_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', r2l_u2r_prob_col]), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first report shows performance of classification for 'normal' and 'R2L&U2R' labels, the second report shows performance for the whole data with adjusted threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_cv_df.filter(r2l_u2r_exp), probCol=r2l_u2r_prob_col, e=0.5, labels=['normal', 'R2L&U2R'])\n",
    "printReport(res_cv_df, probCol=r2l_u2r_prob_col, e=0.05, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df.filter(r2l_u2r_exp), probCol=r2l_u2r_prob_col, e=0.5, labels=['normal', 'R2L&U2R'])\n",
    "printReport(res_test_df, probCol=r2l_u2r_prob_col, e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(r2l_u2r_pred_col, getPrediction(0.05)(col(r2l_u2r_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(r2l_u2r_pred_col, getPrediction(0.01)(col(r2l_u2r_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Combining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sup_prob_col = 'sup_prob'\n",
    "sup_pred_col = 'sup_pred'\n",
    "\n",
    "prob_cols.append(sup_prob_col)\n",
    "pred_cols.append(sup_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_cv_df = res_cv_df.withColumn(sup_prob_col, \n",
    "                                 (col(dos_prob_col) + col(probe_prob_col) + col(r2l_u2r_prob_col))/3).cache()\n",
    "\n",
    "printReport(res_cv_df, sup_prob_col, e=0.05, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_cv_df = res_cv_df.withColumn(sup_pred_col, col(dos_pred_col).cast('int')\n",
    "                                        .bitwiseOR(col(probe_pred_col).cast('int'))\n",
    "                                        .bitwiseOR(col(r2l_u2r_pred_col).cast('int'))).cache()\n",
    "\n",
    "printReport(res_cv_df, sup_pred_col, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_test_df = res_test_df.withColumn(sup_prob_col, \n",
    "                                 (col(dos_prob_col) + col(probe_prob_col) + col(r2l_u2r_prob_col))/3).cache()\n",
    "\n",
    "printReport(res_test_df, sup_prob_col, e=0.005, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_test_df = res_test_df.withColumn(sup_pred_col, col(dos_pred_col).cast('int')\n",
    "                                            .bitwiseOR(col(probe_pred_col).cast('int'))\n",
    "                                            .bitwiseOR(col(r2l_u2r_pred_col).cast('int'))).cache()\n",
    "\n",
    "printReport(res_test_df, sup_pred_col, labels=labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ensembling experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some experiments with ensembling and stacking results from different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Linear combination of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing report of the best single model for comparison\n",
    "printReport(res_test_df, kmeans_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear combination of all models \n",
    "printReport(res_test_df\n",
    "            .select('labels2_index', ((3 * col(kmeans_prob_col) \\\n",
    "                                        + col(gm_prob_col) \\\n",
    "                                        + col(dos_prob_col) \\\n",
    "                                        + col(probe_prob_col) \\\n",
    "                                        + col(r2l_u2r_prob_col))/7)\n",
    "                    .alias('voting')), \n",
    "            'voting', e=0.005, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', ((2 * col(kmeans_prob_col) \\\n",
    "                                        + col(gm_prob_col) \\\n",
    "                                        + col(sup_prob_col))/4)\n",
    "                    .alias('voting')), \n",
    "            'voting', e=0.005, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', (col(kmeans_pred_col).cast('int')\n",
    "                                      .bitwiseOR(col(gm_pred_col).cast('int'))\n",
    "                                      .bitwiseOR(col(sup_pred_col).cast('int')))\n",
    "                    .alias('voting')), \n",
    "                    'voting', labels=labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Logistic Regression and Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "t0 = time()\n",
    "lr_assembler = VectorAssembler(inputCols=[\n",
    "                            kmeans_prob_col, \n",
    "                            gm_prob_col, \n",
    "                            dos_prob_col, \n",
    "                            probe_prob_col, \n",
    "                            r2l_u2r_prob_col\n",
    "                            ], \n",
    "                            outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, labelCol=\"labels2_index\", standardization=False, weightCol='weights')\n",
    "lr_pipeline = Pipeline(stages=[lr_assembler, lr])\n",
    "\n",
    "weights_dict = {\n",
    "    'normal': 1.0,\n",
    "    'DoS': 100.0,\n",
    "    'Probe': 100.0,\n",
    "    'R2L': 100.0,\n",
    "    'U2R': 100.0\n",
    "}\n",
    "\n",
    "udf_weight = udf(lambda row: weights_dict[row], DoubleType())\n",
    "lr_model = lr_pipeline.fit(res_cv_df.withColumn('weights', udf_weight('labels5')))\n",
    "lr_test_df = lr_model.transform(res_test_df).cache()\n",
    "\n",
    "res_test_df = (res_test_df.drop('lr_prob')\n",
    "                    .join(lr_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', 'lr_prob']), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df, 'lr_prob', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "rf_assembler = VectorAssembler(inputCols=[\n",
    "                            kmeans_pred_col, \n",
    "                            gm_pred_col, \n",
    "                            dos_pred_col, \n",
    "                            probe_pred_col, \n",
    "                            r2l_u2r_pred_col\n",
    "                            ],\n",
    "                            outputCol='features')\n",
    "\n",
    "rf_indexer =  VectorIndexer(inputCol='features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='labels2_index', featuresCol='features', seed=seed,\n",
    "                            numTrees=250, maxDepth=5, featureSubsetStrategy='auto')\n",
    "rf_pipeline = Pipeline(stages=[rf_assembler, \n",
    "                               rf_indexer,\n",
    "                               rf])\n",
    "rf_model = rf_pipeline.fit(res_cv_df)\n",
    "rf_test_df = rf_model.transform(res_test_df).cache()\n",
    "\n",
    "res_test_df = (res_test_df.drop('rf_prob')\n",
    "                    .join(rf_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', 'rf_prob']), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df, 'rf_prob', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_test_df = res_test_df.withColumn('lr_pred', getPrediction(0.01)(col('lr_prob'))).cache()\n",
    "res_test_df = res_test_df.withColumn('rf_pred', getPrediction(0.01)(col('rf_prob'))).cache()\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', ((col('lr_prob') + col('rf_prob'))/2)\n",
    "                    .alias('voting')), \n",
    "                    'voting', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', (col('lr_pred').cast('int').bitwiseOR(col('rf_pred').cast('int')))\n",
    "                    .alias('voting')), \n",
    "                    'voting', labels=labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Stacking with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack_cv_df = scaled_cv_df.join(res_cv_df.select('id', *[\n",
    "                            kmeans_pred_col, \n",
    "                            gm_pred_col, \n",
    "                            dos_pred_col, \n",
    "                            probe_pred_col, \n",
    "                            r2l_u2r_pred_col,\n",
    "                            sup_pred_col\n",
    "                            ]), 'id').cache()\n",
    "\n",
    "stack_test_df = scaled_test_df.join(res_test_df.select('id', *[\n",
    "                            kmeans_pred_col, \n",
    "                            gm_pred_col, \n",
    "                            dos_pred_col, \n",
    "                            probe_pred_col, \n",
    "                            r2l_u2r_pred_col,\n",
    "                            sup_pred_col\n",
    "                            ]), 'id').cache()\n",
    "\n",
    "print(stack_cv_df.count())\n",
    "print(stack_test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "pred_assembler = VectorAssembler(inputCols=[\n",
    "                            kmeans_pred_col, \n",
    "                            gm_pred_col, \n",
    "                            dos_pred_col, \n",
    "                            probe_pred_col, \n",
    "                            r2l_u2r_pred_col,\n",
    "                            sup_pred_col\n",
    "                            ], outputCol='pred_features')\n",
    "pred_indexer = VectorIndexer(inputCol='pred_features', outputCol='indexed_pred_features', maxCategories=2)\n",
    "\n",
    "rf_stack_slicer = VectorSlicer(inputCol='indexed_features', outputCol='selected_features', \n",
    "                               names=selectFeaturesByAR(ar_dict, 1.5))\n",
    "\n",
    "rf_stack_assembler = VectorAssembler(inputCols=['selected_features', 'indexed_pred_features'], outputCol='rf_features')\n",
    "\n",
    "rf_stack_classifier = RandomForestClassifier(labelCol=labels_col, featuresCol='rf_features', seed=seed,\n",
    "                                             numTrees=500, maxDepth=20, featureSubsetStrategy=\"auto\")\n",
    "\n",
    "stack_pipeline = Pipeline(stages=[pred_assembler, \n",
    "                                  pred_indexer, \n",
    "                                  rf_stack_slicer, \n",
    "                                  rf_stack_assembler,\n",
    "                                  rf_stack_classifier\n",
    "                                 ])\n",
    "stack_model = stack_pipeline.fit(stack_cv_df)\n",
    "\n",
    "pred_stack_cv_df = stack_model.transform(stack_cv_df).cache()\n",
    "pred_stack_test_df = stack_model.transform(stack_test_df).cache()\n",
    "print(pred_stack_cv_df.count())\n",
    "print(pred_stack_test_df.count())\n",
    "\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "res_cv_df = res_cv_df.drop('prob_stack_rf')\n",
    "res_cv_df = (res_cv_df.join(pred_stack_cv_df.rdd\n",
    "                            .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                            .toDF(['id', 'prob_stack_rf']),\n",
    "                            'id')\n",
    "                        .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "res_test_df = res_test_df.drop('prob_stack_rf')\n",
    "res_test_df = (res_test_df.join(pred_stack_test_df.rdd\n",
    "                            .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                            .toDF(['id', 'prob_stack_rf']),\n",
    "                            'id')\n",
    "                        .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printReport(res_test_df, 'prob_stack_rf', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(time() - gt0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result from a single approach was achieved by KMeans Clustering with Random Forest Classifiers. It gives \n",
    "around ~98-99% of detection rate with reasonable ~14-15% of false alarm rate. F1 score is 0.94, weighted F1 score is 0.93.\n",
    "\n",
    "For improving detection rate ensembling approaches are used. The best of them gives ~99.5-99.6% of detection rate with ~16.1-16.6% of false alarm rate. So there are only about 40-90 attack connections from 12833 (including unknown before) which haven't been recognized."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
